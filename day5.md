# Objectives
1. SVM
2. SVM Example
3. Capstone projects & presentations
4. Summarize supervised and unsupervised
5. Q & A / Wrap-up

# SVM
- Theory of SVM
- [Visual explanation](https://www.youtube.com/watch?v=3liCbRZPrZA)
- Hand-digit recognizer example

# Capstone Projects....
- https://www.kaggle.com/c/whats-cooking-kernels-only/data
- https://www.kaggle.com/c/leaf-classification
- https://www.kaggle.com/c/sf-crime
- https://www.kaggle.com/c/home-credit-default-risk/data
- https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags

# (Optional) PCA
- Why is it useful?

- [Visual explanation]
- [Worksheet](https://s3-us-west-2.amazonaws.com/ga-dat-2015-suneel/worksheets/PCA/PCA_worksheet_1.pdf)

## Practice
- Let's do PCA on a dataset and visualize it
- Let's do PCA on a dataset as preparation for supervised learning

# Summarization
## Naive Bayes
1. What kind of input (categorical/continuous) does the Naive Bayes classifier take?
2. Can the Naive Bayes classifier do multi-class classification?
3. What does the 'Bayes' in Naive Bayes refer to?
4. What does the 'Naive' in Naive refer to?
5. When should we use Naive Bayes?
6. When should we definitely NOT use Naive Bayes?
7. How does Naive Bayes do with high dimensionality?
8. Does this classifier give us an interpretable probability?
9. What parameters can we tune, if any?

## Logistic Regression
0. What is the equation for logistic regression?
1. What kind of input does Logistic take?
2. Can it handle high dimensional input?
3. What is regularization and what is it for?
4. When should we regularize?
6. Does this classifier give us an interpretable probability?
7. What parameters can we tune?

## k Nearest Neighbors
0. What does the k in k Nearest Neighbors refer to?
1. If k is too, high what can happen?
2. Should we scale our features? What happens if we don't?
3. When should we use k Nearest Neighbors?
4. When should we definitely NOT use k Nearest Neighbors?
5. Can k Nearest Neighbors handle high dimensionality?
6. Given a k-neighborhood of points from different classes, say A, B and C, how do we decide which class wins in that neighborhood?
7. What are some ways that we can weight the votes? Why would we want to do so?
8. What parameters can we tune?
9. Do we get an interpretable probability?

## Dataset 1
- We have a set of historical brain scans, each tagged as either Benign or Cancerous.
- There are ~1,000 features
- There are 100,000 samples
- We want to give a patient the probability they have cancer.

What method(s) would you try and why?

## Dataset 2
- We have a set of legal documents and we need to classify them as either type A, B, C or D.
- There are ~5,000 words in each document
- There are 80,000 samples

What method(s) would you try and why?

## Dataset 3
- We have collected some data for a number of different store fronts and have tagged each storefront as either Successful or Unsuccessful
- There are 10 features
- There are ~1000 samples
- Give a new measurement, we want to place it in eiter the Successful or Unsuccessful bucket and attach a probability to it.

What method(s) would you try and why?



